apiVersion: cluster.x-k8s.io/v1alpha3
kind: Cluster
metadata:
  labels:
    cni: calico
  name: ${CLUSTER_NAME}
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1alpha3
    kind: KubeadmControlPlane
    name: ${CLUSTER_NAME}-control-plane
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
    kind: AzureCluster
    name: ${CLUSTER_NAME}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
kind: AzureCluster
metadata:
  name: ${CLUSTER_NAME}
  namespace: default
spec:
  location: ${AZURE_LOCATION}
  networkSpec:
    vnet:
      name: ${AZURE_VNET_NAME:=${CLUSTER_NAME}-vnet}
  resourceGroup: ${AZURE_RESOURCE_GROUP:=${CLUSTER_NAME}}
  subscriptionID: ${AZURE_SUBSCRIPTION_ID}
---
apiVersion: controlplane.cluster.x-k8s.io/v1alpha3
kind: KubeadmControlPlane
metadata:
  name: ${CLUSTER_NAME}-control-plane
  namespace: default
spec:
  infrastructureTemplate:
    apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
    kind: AzureMachineTemplate
    name: ${CLUSTER_NAME}-control-plane
  kubeadmConfigSpec:
    clusterConfiguration:
      apiServer:
        extraArgs:
          cloud-config: /etc/kubernetes/azure.json
          cloud-provider: azure
        extraVolumes:
        - hostPath: /etc/kubernetes/azure.json
          mountPath: /etc/kubernetes/azure.json
          name: cloud-config
          readOnly: true
        timeoutForControlPlane: 20m
      controllerManager:
        extraArgs:
          allocate-node-cidrs: "false"
          cloud-config: /etc/kubernetes/azure.json
          cloud-provider: azure
          cluster-name: ${CLUSTER_NAME}
        extraVolumes:
        - hostPath: /etc/kubernetes/azure.json
          mountPath: /etc/kubernetes/azure.json
          name: cloud-config
          readOnly: true
      etcd:
        local:
          dataDir: /var/lib/etcddisk/etcd
    diskSetup:
      filesystems:
      - device: /dev/disk/azure/scsi1/lun0
        extraOpts:
        - -E
        - lazy_itable_init=1,lazy_journal_init=1
        filesystem: ext4
        label: etcd_disk
      - device: ephemeral0.1
        filesystem: ext4
        label: ephemeral0
        replaceFS: ntfs
      partitions:
      - device: /dev/disk/azure/scsi1/lun0
        layout: true
        overwrite: false
        tableType: gpt
    files:
    - contentFrom:
        secret:
          key: control-plane-azure.json
          name: ${CLUSTER_NAME}-control-plane-azure-json
      owner: root:root
      path: /etc/kubernetes/azure.json
      permissions: "0644"
    initConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-config: /etc/kubernetes/azure.json
          cloud-provider: azure
        name: '{{ ds.meta_data["local_hostname"] }}'
    joinConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-config: /etc/kubernetes/azure.json
          cloud-provider: azure
        name: '{{ ds.meta_data["local_hostname"] }}'
    mounts:
    - - LABEL=etcd_disk
      - /var/lib/etcddisk
    postKubeadmCommands:
    - KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/master/nvidia-device-plugin.yml
    useExperimentalRetryJoin: true
  replicas: ${CONTROL_PLANE_MACHINE_COUNT}
  version: ${KUBERNETES_VERSION}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
kind: AzureMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-control-plane
  namespace: default
spec:
  template:
    spec:
      dataDisks:
      - diskSizeGB: 256
        lun: 0
        nameSuffix: etcddisk
      location: ${AZURE_LOCATION}
      osDisk:
        diskSizeGB: 128
        managedDisk:
          storageAccountType: Premium_LRS
        osType: Linux
      sshPublicKey: ${AZURE_SSH_PUBLIC_KEY_B64:=""}
      vmSize: ${AZURE_CONTROL_PLANE_MACHINE_TYPE}
---
apiVersion: cluster.x-k8s.io/v1alpha3
kind: MachineDeployment
metadata:
  name: ${CLUSTER_NAME}-md-0
  namespace: default
spec:
  clusterName: ${CLUSTER_NAME}
  replicas: ${WORKER_MACHINE_COUNT}
  selector:
    matchLabels: null
  template:
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1alpha3
          kind: KubeadmConfigTemplate
          name: ${CLUSTER_NAME}-md-0
      clusterName: ${CLUSTER_NAME}
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
        kind: AzureMachineTemplate
        name: ${CLUSTER_NAME}-md-0
      version: ${KUBERNETES_VERSION}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
kind: AzureMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-md-0
  namespace: default
spec:
  template:
    spec:
      location: ${AZURE_LOCATION}
      osDisk:
        diskSizeGB: 128
        managedDisk:
          storageAccountType: Premium_LRS
        osType: Linux
      sshPublicKey: ${AZURE_SSH_PUBLIC_KEY_B64:=""}
      vmSize: ${AZURE_NODE_MACHINE_TYPE}
---
apiVersion: bootstrap.cluster.x-k8s.io/v1alpha3
kind: KubeadmConfigTemplate
metadata:
  name: ${CLUSTER_NAME}-md-0
  namespace: default
spec:
  template:
    spec:
      files:
      - contentFrom:
          secret:
            key: worker-node-azure.json
            name: ${CLUSTER_NAME}-md-0-azure-json
        owner: root:root
        path: /etc/kubernetes/azure.json
        permissions: "0644"
      - content: |
          version = 2
          root = "/var/lib/containerd"
          state = "/run/containerd"
          plugin_dir = ""
          disabled_plugins = []
          required_plugins = []
          oom_score = 0

          [grpc]
            address = "/run/containerd/containerd.sock"
            tcp_address = ""
            tcp_tls_cert = ""
            tcp_tls_key = ""
            uid = 0
            gid = 0
            max_recv_message_size = 16777216
            max_send_message_size = 16777216

          [ttrpc]
            address = ""
            uid = 0
            gid = 0

          [debug]
            address = ""
            uid = 0
            gid = 0
            level = ""

          [metrics]
            address = ""
            grpc_histogram = false

          [cgroup]
            path = ""

          [timeouts]
            "io.containerd.timeout.shim.cleanup" = "5s"
            "io.containerd.timeout.shim.load" = "5s"
            "io.containerd.timeout.shim.shutdown" = "3s"
            "io.containerd.timeout.task.state" = "2s"

          [plugins]
            [plugins."io.containerd.gc.v1.scheduler"]
              pause_threshold = 0.02
              deletion_threshold = 0
              mutation_threshold = 100
              schedule_delay = "0s"
              startup_delay = "100ms"
            [plugins."io.containerd.grpc.v1.cri"]
              disable_tcp_service = true
              stream_server_address = "127.0.0.1"
              stream_server_port = "0"
              stream_idle_timeout = "4h0m0s"
              enable_selinux = false
              sandbox_image = "k8s.gcr.io/pause:3.1"
              stats_collect_period = 10
              systemd_cgroup = false
              enable_tls_streaming = false
              max_container_log_line_size = 16384
              disable_cgroup = false
              disable_apparmor = false
              restrict_oom_score_adj = false
              max_concurrent_downloads = 3
              disable_proc_mount = false
              [plugins."io.containerd.grpc.v1.cri".containerd]
                snapshotter = "overlayfs"
                default_runtime_name = "nvidia-container-runtime"
                no_pivot = false
                [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime]
                  runtime_type = ""
                  runtime_engine = ""
                  runtime_root = ""
                  privileged_without_host_devices = false
                [plugins."io.containerd.grpc.v1.cri".containerd.untrusted_workload_runtime]
                  runtime_type = ""
                  runtime_engine = ""
                  runtime_root = ""
                  privileged_without_host_devices = false
                [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]
                  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
                    runtime_type = "io.containerd.runc.v1"
                    runtime_engine = ""
                    runtime_root = ""
                    privileged_without_host_devices = false
                  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia-container-runtime]
                    runtime_type = "io.containerd.runc.v1"
                      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia-container-runtime.options]
                        BinaryName = "nvidia-container-runtime"
              [plugins."io.containerd.grpc.v1.cri".cni]
                bin_dir = "/opt/cni/bin"
                conf_dir = "/etc/cni/net.d"
                max_conf_num = 1
                conf_template = ""
              [plugins."io.containerd.grpc.v1.cri".registry]
                [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
                  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
                    endpoint = ["https://registry-1.docker.io"]
              [plugins."io.containerd.grpc.v1.cri".x509_key_pair_streaming]
                tls_cert_file = ""
                tls_key_file = ""
            [plugins."io.containerd.internal.v1.opt"]
              path = "/opt/containerd"
            [plugins."io.containerd.internal.v1.restart"]
              interval = "10s"
            [plugins."io.containerd.metadata.v1.bolt"]
              content_sharing_policy = "shared"
            [plugins."io.containerd.monitor.v1.cgroups"]
              no_prometheus = false
            [plugins."io.containerd.runtime.v1.linux"]
              shim = "containerd-shim"
              runtime = "nvidia-container-runtime"
              runtime_root = ""
              no_shim = false
              shim_debug = false
            [plugins."io.containerd.runtime.v2.task"]
              platforms = ["linux/amd64"]
            [plugins."io.containerd.service.v1.diff-service"]
              default = ["walking"]
            [plugins."io.containerd.snapshotter.v1.devmapper"]
              root_path = ""
              pool_name = ""
              base_image_size = ""
        owner: root:root
        path: /etc/containerd/nvidia-config.toml
        permissions: "0644"
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            cloud-config: /etc/kubernetes/azure.json
            cloud-provider: azure
          name: '{{ ds.meta_data["local_hostname"] }}'
      postKubeadmCommands:
      - cp /etc/containerd/config.toml /etc/containerd/config.toml.old
      - mv /etc/containerd/nvidia-config.toml /etc/containerd/config.toml
      - systemctl restart containerd
      preKubeadmCommands:
      - curl -sL https://nvidia.github.io/nvidia-container-runtime/gpgkey | apt-key
        add -
      - curl -sL https://nvidia.github.io/nvidia-container-runtime/$(. /etc/os-release;echo
        $ID$VERSION_ID)/nvidia-container-runtime.list | tee /etc/apt/sources.list.d/nvidia-container-runtime.list
      - apt update
      - apt install ubuntu-drivers-common -y
      - ubuntu-drivers install
      - apt install nvidia-container-runtime -y
      useExperimentalRetryJoin: true
