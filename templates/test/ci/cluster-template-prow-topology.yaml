apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cloud-provider: ${CLOUD_PROVIDER_AZURE_LABEL:=azure}
    cni: calico
    containerd-logger: enabled
    csi-proxy: enabled
  name: ${CLUSTER_NAME}
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  topology:
    class: ${CLUSTER_CLASS_NAME}
    controlPlane:
      replicas: ${CONTROL_PLANE_MACHINE_COUNT:=1}
    variables:
    - name: subscriptionID
      value: ${AZURE_SUBSCRIPTION_ID}
    - name: controlPlaneMachineType
      value: ${AZURE_CONTROL_PLANE_MACHINE_TYPE:-""}
    - name: workerMachineType
      value: ${AZURE_NODE_MACHINE_TYPE:-""}
    - name: sshPublicKey
      value: ${AZURE_SSH_PUBLIC_KEY_B64:-""}
    - name: buildProvenance
      value: ${BUILD_PROVENANCE:-""}
    - name: timestamp
      value: ${TIMESTAMP:-""}
    - name: jobName
      value: ${JOB_NAME:-""}
    - name: clusterIdentityRef
      value: ${CLUSTER_IDENTITY_NAME}
    - name: location
      value: ${AZURE_LOCATION}
    - name: k8sFeatureGates
      value: ${K8S_FEATURE_GATES:-""}
    - name: logLevel
      value: "4"
    version: ${KUBERNETES_VERSION}
    workers:
      machineDeployments:
      - class: ${CLUSTER_NAME}-worker
        name: md-0
        replicas: ${WORKER_MACHINE_COUNT}
---
apiVersion: addons.cluster.x-k8s.io/v1alpha1
kind: HelmChartProxy
metadata:
  name: calico
  namespace: default
spec:
  chartName: tigera-operator
  clusterSelector:
    matchLabels:
      cni: calico
  namespace: tigera-operator
  releaseName: projectcalico
  repoURL: https://docs.tigera.io/calico/charts
  valuesTemplate: |
    installation:
      cni:
        type: Calico
        ipam:
          type: Calico
      calicoNetwork:
        bgp: Disabled
        windowsDataplane: HNS
        mtu: 1350
        ipPools:{{range $i, $cidr := .Cluster.spec.clusterNetwork.pods.cidrBlocks }}
        - cidr: {{ $cidr }}
          encapsulation: VXLAN{{end}}
      typhaDeployment:
        spec:
          template:
            spec:
              # By default, typha tolerates all NoSchedule taints. This breaks
              # scale-ins when it continuously gets scheduled onto an
              # out-of-date Node that is being deleted. Tolerate only the
              # NoSchedule taints that are expected.
              tolerations:
                - effect: NoExecute
                  operator: Exists
                - effect: NoSchedule
                  key: node-role.kubernetes.io/control-plane
                  operator: Exists
                - effect: NoSchedule
                  key: node.kubernetes.io/not-ready
                  operator: Exists
              affinity:
                nodeAffinity:
                  preferredDuringSchedulingIgnoredDuringExecution:
                  - weight: 50
                    preference:
                      matchExpressions:
                      - key: node-role.kubernetes.io/control-plane
                        operator: Exists
      registry: capzcicommunity.azurecr.io
      serviceCIDRs:
        - 10.96.0.0/12 # must match cluster service CIDR (this is the default)
    # Image and registry configuration for the tigera/operator pod
    tigeraOperator:
      image: tigera/operator
      registry: capzcicommunity.azurecr.io
    calicoctl:
      image: capzcicommunity.azurecr.io/calico/ctl
    # when kubernetesServiceEndpoint (required for windows) is added
    # DNS configuration is needed to look up the api server name properly
    # https://github.com/projectcalico/calico/issues/9536
    dnsConfig:
      nameservers:
        - 127.0.0.53
      options:
        - name: edns0
        - name: trust-ad
    kubernetesServiceEndpoint:
      host: "{{ .Cluster.spec.controlPlaneEndpoint.host }}"
      port: "{{ .Cluster.spec.controlPlaneEndpoint.port }}"
    # By default, tigera tolerates all NoSchedule taints. This breaks upgrades
    # when it continuously gets scheduled onto an out-of-date Node that is being
    # deleted. Tolerate only the NoSchedule taints that are expected.
    tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        key: node-role.kubernetes.io/control-plane
        operator: Exists
      - effect: NoSchedule
        key: node.kubernetes.io/not-ready
        operator: Exists
  version: ${CALICO_VERSION}
---
apiVersion: addons.cluster.x-k8s.io/v1alpha1
kind: HelmChartProxy
metadata:
  name: azuredisk-csi-driver-chart
  namespace: default
spec:
  chartName: azuredisk-csi-driver
  clusterSelector:
    matchLabels:
      azuredisk-csi: "true"
  namespace: kube-system
  releaseName: azuredisk-csi-driver-oot
  repoURL: https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/charts
  valuesTemplate: |-
    controller:
      replicas: 1
      runOnControlPlane: true
    windows:
      useHostProcessContainers: {{ hasKey .Cluster.metadata.labels "cni-windows" }}
---
apiVersion: addons.cluster.x-k8s.io/v1alpha1
kind: HelmChartProxy
metadata:
  name: cloud-provider-azure-chart
  namespace: default
spec:
  chartName: cloud-provider-azure
  clusterSelector:
    matchLabels:
      cloud-provider: azure
  releaseName: cloud-provider-azure-oot
  repoURL: https://raw.githubusercontent.com/kubernetes-sigs/cloud-provider-azure/master/helm/repo
  valuesTemplate: |
    infra:
      clusterName: {{ .Cluster.metadata.name }}
    cloudControllerManager:
      clusterCIDR: {{ .Cluster.spec.clusterNetwork.pods.cidrBlocks | join "," }}
      logVerbosity: 4
---
apiVersion: addons.cluster.x-k8s.io/v1alpha1
kind: HelmChartProxy
metadata:
  name: cloud-provider-azure-chart-ci
  namespace: default
spec:
  chartName: cloud-provider-azure
  clusterSelector:
    matchLabels:
      cloud-provider: azure-ci
  releaseName: cloud-provider-azure-oot
  repoURL: https://raw.githubusercontent.com/kubernetes-sigs/cloud-provider-azure/master/helm/repo
  valuesTemplate: |
    infra:
      clusterName: {{ .Cluster.metadata.name }}
    cloudControllerManager:
      cloudConfig: ${CLOUD_CONFIG:-"/etc/kubernetes/azure.json"}
      cloudConfigSecretName: ${CONFIG_SECRET_NAME:-""}
      clusterCIDR: {{ .Cluster.spec.clusterNetwork.pods.cidrBlocks | join "," }}
      imageName: "${CCM_IMAGE_NAME:-""}"
      imageRepository: "${IMAGE_REGISTRY:-""}"
      imageTag: "${IMAGE_TAG_CCM:-""}"
      logVerbosity: ${CCM_LOG_VERBOSITY:-4}
      replicas: ${CCM_COUNT:-1}
      enableDynamicReloading: ${ENABLE_DYNAMIC_RELOADING:-false}
    cloudNodeManager:
      imageName: "${CNM_IMAGE_NAME:-""}"
      imageRepository: "${IMAGE_REGISTRY:-""}"
      imageTag: "${IMAGE_TAG_CNM:-""}"
