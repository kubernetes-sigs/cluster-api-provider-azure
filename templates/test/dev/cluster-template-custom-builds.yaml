apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    containerd-logger: enabled
    csi-proxy: enabled
    metrics-server: enabled
  name: ${CLUSTER_NAME}
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: ${CLUSTER_NAME}-control-plane
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: AzureCluster
    name: ${CLUSTER_NAME}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AzureCluster
metadata:
  name: ${CLUSTER_NAME}
  namespace: default
spec:
  additionalTags:
    buildProvenance: ${BUILD_PROVENANCE}
    creationTimestamp: ${TIMESTAMP}
    jobName: ${JOB_NAME}
  identityRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: AzureClusterIdentity
    name: ${CLUSTER_IDENTITY_NAME}
  location: ${AZURE_LOCATION}
  networkSpec:
    subnets:
    - name: control-plane-subnet
      role: control-plane
    - name: node-subnet
      natGateway:
        name: node-natgateway
      role: node
    vnet:
      name: ${AZURE_VNET_NAME:=${CLUSTER_NAME}-vnet}
  resourceGroup: ${AZURE_RESOURCE_GROUP:=${CLUSTER_NAME}}
  subscriptionID: ${AZURE_SUBSCRIPTION_ID}
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  annotations:
    controlplane.cluster.x-k8s.io/skip-kube-proxy: "true"
  name: ${CLUSTER_NAME}-control-plane
  namespace: default
spec:
  kubeadmConfigSpec:
    clusterConfiguration:
      apiServer:
        extraArgs:
          cloud-config: /etc/kubernetes/azure.json
          cloud-provider: azure
          feature-gates: ${K8S_FEATURE_GATES:-""}
        extraVolumes:
        - hostPath: /etc/kubernetes/azure.json
          mountPath: /etc/kubernetes/azure.json
          name: cloud-config
          readOnly: true
        timeoutForControlPlane: 20m
      controllerManager:
        extraArgs:
          allocate-node-cidrs: "false"
          cloud-config: /etc/kubernetes/azure.json
          cloud-provider: azure
          cluster-name: ${CLUSTER_NAME}
          feature-gates: HPAContainerMetrics=true
          v: "4"
        extraVolumes:
        - hostPath: /etc/kubernetes/azure.json
          mountPath: /etc/kubernetes/azure.json
          name: cloud-config
          readOnly: true
      etcd:
        local:
          dataDir: /var/lib/etcddisk/etcd
          extraArgs:
            quota-backend-bytes: "8589934592"
      kubernetesVersion: ci/${CI_VERSION}
    diskSetup:
      filesystems:
      - device: /dev/disk/azure/scsi1/lun0
        extraOpts:
        - -E
        - lazy_itable_init=1,lazy_journal_init=1
        filesystem: ext4
        label: etcd_disk
      - device: ephemeral0.1
        filesystem: ext4
        label: ephemeral0
        replaceFS: ntfs
      partitions:
      - device: /dev/disk/azure/scsi1/lun0
        layout: true
        overwrite: false
        tableType: gpt
    files:
    - contentFrom:
        secret:
          key: control-plane-azure.json
          name: ${CLUSTER_NAME}-control-plane-azure-json
      owner: root:root
      path: /etc/kubernetes/azure.json
      permissions: "0644"
    - content: |
        #!/bin/bash

        set -o nounset
        set -o pipefail
        set -o errexit

        systemctl stop kubelet
        declare -a BINARIES=("kubeadm" "kubectl" "kubelet")
        for BINARY in "$${BINARIES[@]}"; do
          echo "* installing package: $${BINARY} ${KUBE_GIT_VERSION}"
          curl --retry 10 --retry-delay 5 "https://${AZURE_STORAGE_ACCOUNT}.blob.core.windows.net/${JOB_NAME}/${KUBE_GIT_VERSION}/bin/linux/amd64/$${BINARY}" --output "/usr/bin/$${BINARY}"
        done
        systemctl restart kubelet

        # prepull images from gcr.io/k8s-staging-ci-images and retag it to
        # k8s.gcr.io so kubeadm can fetch correct images no matter what
        declare -a IMAGES=("kube-apiserver" "kube-controller-manager" "kube-proxy" "kube-scheduler")
        [[ $(id -u) != 0 ]] && SUDO="sudo" || SUDO=""
        for IMAGE in "$${IMAGES[@]}"; do
          $${SUDO} ctr -n k8s.io images pull "gcr.io/k8s-staging-ci-images/$${IMAGE}:${CI_VERSION/+/_}"
          $${SUDO} ctr -n k8s.io images tag "gcr.io/k8s-staging-ci-images/$${IMAGE}:${CI_VERSION/+/_}" "k8s.gcr.io/$${IMAGE}:${CI_VERSION/+/_}"
        done

        echo "kubeadm version: $(kubeadm version -o=short)"
        echo "kubectl version: $(kubectl version --client=true --short=true)"
        echo "kubelet version: $(kubelet --version)"
      owner: root:root
      path: /tmp/replace-k8s-binaries.sh
      permissions: "0744"
    - content: |
        #!/bin/bash

        set -o nounset
        set -o pipefail
        set -o errexit

        curl -L --retry 10 --retry-delay 5 https://github.com/mikefarah/yq/releases/download/v4.6.1/yq_linux_amd64.tar.gz --output /tmp/yq_linux_amd64.tar.gz
        tar -xzvf /tmp/yq_linux_amd64.tar.gz -C /tmp && mv /tmp/yq_linux_amd64 /usr/bin/yq
        rm /tmp/yq_linux_amd64.tar.gz

        export KUBECONFIG=/etc/kubernetes/admin.conf
        kubectl -n kube-system set image daemonset/kube-proxy kube-proxy="${REGISTRY}/kube-proxy:${IMAGE_TAG}"
        yq e '.spec.containers[0].image = "${REGISTRY}/kube-apiserver:${IMAGE_TAG}"' -i /etc/kubernetes/manifests/kube-apiserver.yaml
        yq e '.spec.containers[0].image = "${REGISTRY}/kube-controller-manager:${IMAGE_TAG}"' -i /etc/kubernetes/manifests/kube-controller-manager.yaml
        yq e '.spec.containers[0].image = "${REGISTRY}/kube-scheduler:${IMAGE_TAG}"' -i /etc/kubernetes/manifests/kube-scheduler.yaml
      owner: root:root
      path: /tmp/replace-k8s-components.sh
      permissions: "0744"
    initConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          azure-container-registry-config: /etc/kubernetes/azure.json
          cloud-config: /etc/kubernetes/azure.json
          cloud-provider: azure
        name: '{{ ds.meta_data["local_hostname"] }}'
    joinConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          azure-container-registry-config: /etc/kubernetes/azure.json
          cloud-config: /etc/kubernetes/azure.json
          cloud-provider: azure
        name: '{{ ds.meta_data["local_hostname"] }}'
    mounts:
    - - LABEL=etcd_disk
      - /var/lib/etcddisk
    postKubeadmCommands:
    - bash -c /tmp/replace-k8s-components.sh
    preKubeadmCommands:
    - bash -c /tmp/replace-k8s-binaries.sh
    useExperimentalRetryJoin: true
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: AzureMachineTemplate
      name: ${CLUSTER_NAME}-control-plane
  replicas: ${CONTROL_PLANE_MACHINE_COUNT}
  version: ${KUBERNETES_VERSION}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AzureMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-control-plane
  namespace: default
spec:
  template:
    spec:
      dataDisks:
      - diskSizeGB: 256
        lun: 0
        nameSuffix: etcddisk
      image:
        marketplace:
          offer: capi
          publisher: cncf-upstream
          sku: k8s-1dot18dot8-ubuntu-1804
          version: 2020.08.17
      osDisk:
        diskSizeGB: 128
        osType: Linux
      sshPublicKey: ${AZURE_SSH_PUBLIC_KEY_B64:=""}
      vmSize: ${AZURE_CONTROL_PLANE_MACHINE_TYPE}
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: ${CLUSTER_NAME}-md-0
  namespace: default
spec:
  clusterName: ${CLUSTER_NAME}
  replicas: ${WORKER_MACHINE_COUNT}
  selector: {}
  template:
    metadata:
      labels:
        nodepool: pool1
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: ${CLUSTER_NAME}-md-0
      clusterName: ${CLUSTER_NAME}
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: AzureMachineTemplate
        name: ${CLUSTER_NAME}-md-0
      version: ${KUBERNETES_VERSION}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AzureMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-md-0
  namespace: default
spec:
  template:
    spec:
      image:
        marketplace:
          offer: capi
          publisher: cncf-upstream
          sku: k8s-1dot18dot8-ubuntu-1804
          version: 2020.08.17
      osDisk:
        diskSizeGB: 128
        osType: Linux
      sshPublicKey: ${AZURE_SSH_PUBLIC_KEY_B64:=""}
      vmSize: ${AZURE_NODE_MACHINE_TYPE}
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: ${CLUSTER_NAME}-md-0
  namespace: default
spec:
  template:
    spec:
      files:
      - contentFrom:
          secret:
            key: worker-node-azure.json
            name: ${CLUSTER_NAME}-md-0-azure-json
        owner: root:root
        path: /etc/kubernetes/azure.json
        permissions: "0644"
      - content: |
          #!/bin/bash

          set -o nounset
          set -o pipefail
          set -o errexit

          systemctl stop kubelet
          declare -a BINARIES=("kubeadm" "kubectl" "kubelet")
          for BINARY in "$${BINARIES[@]}"; do
            echo "* installing package: $${BINARY} ${KUBE_GIT_VERSION}"
            curl --retry 10 --retry-delay 5 "https://${AZURE_STORAGE_ACCOUNT}.blob.core.windows.net/${JOB_NAME}/${KUBE_GIT_VERSION}/bin/linux/amd64/$${BINARY}" --output "/usr/bin/$${BINARY}"
          done
          systemctl restart kubelet

          echo "kubeadm version: $(kubeadm version -o=short)"
          echo "kubectl version: $(kubectl version --client=true --short=true)"
          echo "kubelet version: $(kubelet --version)"
        owner: root:root
        path: /tmp/replace-k8s-binaries.sh
        permissions: "0744"
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            azure-container-registry-config: /etc/kubernetes/azure.json
            cloud-config: /etc/kubernetes/azure.json
            cloud-provider: azure
          name: '{{ ds.meta_data["local_hostname"] }}'
      preKubeadmCommands:
      - bash -c /tmp/replace-k8s-binaries.sh
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: ${CLUSTER_NAME}-md-win
  namespace: default
spec:
  clusterName: ${CLUSTER_NAME}
  replicas: ${WINDOWS_WORKER_MACHINE_COUNT:-0}
  selector: {}
  template:
    metadata:
      labels:
        windows-healthcheck: "true"
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: ${CLUSTER_NAME}-md-win
      clusterName: ${CLUSTER_NAME}
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: AzureMachineTemplate
        name: ${CLUSTER_NAME}-md-win
      version: ${KUBERNETES_VERSION}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AzureMachineTemplate
metadata:
  annotations:
    runtime: containerd
  name: ${CLUSTER_NAME}-md-win
  namespace: default
spec:
  template:
    metadata:
      annotations:
        runtime: containerd
        windowsServerVersion: ${WINDOWS_SERVER_VERSION:=""}
    spec:
      image:
        marketplace:
          offer: capi-windows
          publisher: cncf-upstream
          sku: k8s-1dot23dot4-windows-2019-containerd
          version: latest
      osDisk:
        diskSizeGB: 128
        managedDisk:
          storageAccountType: Premium_LRS
        osType: Windows
      sshPublicKey: ${AZURE_SSH_PUBLIC_KEY_B64:=""}
      vmSize: ${AZURE_NODE_MACHINE_TYPE}
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: ${CLUSTER_NAME}-md-win
  namespace: default
spec:
  template:
    spec:
      files:
      - contentFrom:
          secret:
            key: worker-node-azure.json
            name: ${CLUSTER_NAME}-md-win-azure-json
        owner: root:root
        path: c:/k/azure.json
        permissions: "0644"
      - content: |-
          Add-MpPreference -ExclusionProcess C:/opt/cni/bin/calico.exe
          Add-MpPreference -ExclusionProcess C:/opt/cni/bin/calico-ipam.exe
        path: C:/defender-exclude-calico.ps1
        permissions: "0744"
      - content: |
          # /tmp is assumed created and required for upstream e2e tests to pass
          New-Item -ItemType Directory -Force -Path C:\tmp\
        path: C:/create-temp-folder.ps1
        permissions: "0744"
      - content: |
          $ErrorActionPreference = 'Stop'

          $$CONTAINERD_URL="${WINDOWS_CONTAINERD_URL}"
          if($$CONTAINERD_URL -ne ""){
            # Kubelet service depends on contianerd service so make a best effort attempt to stop it
            Stop-Service kubelet -Force -ErrorAction SilentlyContinue
            Stop-Service containerd -Force
            echo "downloading containerd: $$CONTAINERD_URL"
            curl.exe --retry 10 --retry-delay 5 -L "$$CONTAINERD_URL" --output "c:/k/containerd.tar.gz"
            tar.exe -zxvf c:/k/containerd.tar.gz -C "c:/Program Files/containerd" --strip-components 1

            Start-Service containerd
          }

          containerd.exe --version
          containerd-shim-runhcs-v1.exe --version
        path: C:/replace-containerd.ps1
        permissions: "0744"
      - content: |
          $ErrorActionPreference = 'Stop'

          Stop-Service kubelet -Force

          $$CI_VERSION="${CI_VERSION}"
          if($$CI_VERSION -ne "")
          {
            $$binaries=@("kubeadm", "kubectl", "kubelet", "kube-proxy")
            $$ci_url="https://storage.googleapis.com/k8s-release-dev/ci/$$CI_VERSION/bin/windows/amd64"
            foreach ( $$binary in $$binaries )
            {
              echo "downloading binary: $$ci_url/$$binary.exe"
              curl.exe --retry 10 --retry-delay 5 "$$ci_url/$$binary.exe" --output "c:/k/$$binary.exe"
            }
          }

          # Tag it to the ci version.  The image knows how to use the copy locally with the configmap
          # that is applied at at this stage (windows-kubeproxy-ci.yaml)
          ctr.exe -n k8s.io images pull docker.io/sigwindowstools/kube-proxy:v1.23.1-calico-hostprocess
          ctr.exe -n k8s.io images tag docker.io/sigwindowstools/kube-proxy:v1.23.1-calico-hostprocess "docker.io/sigwindowstools/kube-proxy:${CI_VERSION/+/_}-calico-hostprocess"

          kubeadm.exe version -o=short
          kubectl.exe version --client=true --short=true
          kubelet.exe --version
          kube-proxy.exe --version
        path: C:/replace-ci-binaries.ps1
        permissions: "0744"
      joinConfiguration:
        nodeRegistration:
          criSocket: npipe:////./pipe/containerd-containerd
          kubeletExtraArgs:
            azure-container-registry-config: c:/k/azure.json
            cloud-config: c:/k/azure.json
            cloud-provider: azure
            feature-gates: WindowsHostProcessContainers=true
            v: "2"
            windows-priorityclass: ABOVE_NORMAL_PRIORITY_CLASS
          name: '{{ ds.meta_data["local_hostname"] }}'
      postKubeadmCommands:
      - nssm set kubelet start SERVICE_AUTO_START
      - powershell C:/defender-exclude-calico.ps1
      preKubeadmCommands:
      - powershell C:/create-temp-folder.ps1
      - powershell C:/replace-containerd.ps1
      - powershell C:/replace-ci-binaries.ps1
      users:
      - groups: Administrators
        name: capi
        sshAuthorizedKeys:
        - ${AZURE_SSH_PUBLIC_KEY:=""}
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: ${CLUSTER_NAME}-mhc-0
  namespace: default
spec:
  clusterName: ${CLUSTER_NAME}
  maxUnhealthy: 100%
  selector:
    matchLabels:
      nodepool: pool1
  unhealthyConditions:
  - status: "True"
    timeout: 30s
    type: E2ENodeUnhealthy
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: ${CLUSTER_NAME}-mhc-windows
  namespace: default
spec:
  clusterName: ${CLUSTER_NAME}
  maxUnhealthy: 100%
  nodeStartupTimeout: 10m
  selector:
    matchLabels:
      windows-healthcheck: "true"
  unhealthyConditions:
  - status: "false"
    timeout: 300s
    type: Ready
  - status: Unknown
    timeout: 300s
    type: Ready
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AzureClusterIdentity
metadata:
  labels:
    clusterctl.cluster.x-k8s.io/move-hierarchy: "true"
  name: ${CLUSTER_IDENTITY_NAME}
  namespace: default
spec:
  allowedNamespaces: {}
  clientID: ${AZURE_CLIENT_ID}
  clientSecret:
    name: ${AZURE_CLUSTER_IDENTITY_SECRET_NAME}
    namespace: ${AZURE_CLUSTER_IDENTITY_SECRET_NAMESPACE}
  tenantID: ${AZURE_TENANT_ID}
  type: ServicePrincipal
---
apiVersion: addons.cluster.x-k8s.io/v1beta1
kind: ClusterResourceSet
metadata:
  name: csi-proxy
  namespace: default
spec:
  clusterSelector:
    matchLabels:
      csi-proxy: enabled
  resources:
  - kind: ConfigMap
    name: csi-proxy-addon
  strategy: ApplyOnce
---
apiVersion: addons.cluster.x-k8s.io/v1beta1
kind: ClusterResourceSet
metadata:
  name: containerd-logger-${CLUSTER_NAME}
  namespace: default
spec:
  clusterSelector:
    matchLabels:
      containerd-logger: enabled
  resources:
  - kind: ConfigMap
    name: containerd-logger-${CLUSTER_NAME}
  strategy: ApplyOnce
---
apiVersion: v1
data:
  csi-proxy: |
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      labels:
        k8s-app: csi-proxy
      name: csi-proxy
      namespace: kube-system
    spec:
      selector:
        matchLabels:
          k8s-app: csi-proxy
      template:
        metadata:
          labels:
            k8s-app: csi-proxy
        spec:
          nodeSelector:
            "kubernetes.io/os": windows
          securityContext:
            windowsOptions:
              hostProcess: true
              runAsUserName: "NT AUTHORITY\\SYSTEM"
          hostNetwork: true
          containers:
            - name: csi-proxy
              image: ghcr.io/kubernetes-sigs/sig-windows/csi-proxy:v1.0.2
kind: ConfigMap
metadata:
  annotations:
    note: generated
  labels:
    type: generated
  name: csi-proxy-addon
  namespace: default
---
apiVersion: v1
data:
  containerd-windows-logger: |
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      labels:
        k8s-app: containerd-logger
      name: containerd-logger
      namespace: kube-system
    spec:
      selector:
        matchLabels:
          k8s-app: containerd-logger
      template:
        metadata:
          labels:
            k8s-app: containerd-logger
        spec:
          securityContext:
            windowsOptions:
              hostProcess: true
              runAsUserName: "NT AUTHORITY\\system"
          hostNetwork: true
          containers:
          - image: ghcr.io/kubernetes-sigs/sig-windows/eventflow-logger:v0.1.0
            args: [ "config.json" ]
            name: containerd-logger
            imagePullPolicy: Always
            volumeMounts:
            - name: containerd-logger-config
              mountPath: /config.json
              subPath: config.json
          nodeSelector:
            kubernetes.io/os: windows
          tolerations:
          - key: CriticalAddonsOnly
            operator: Exists
          - operator: Exists
          volumes:
          - configMap:
              name: containerd-logger-config
            name: containerd-logger-config
      updateStrategy:
        type: RollingUpdate
    ---
    kind: ConfigMap
    apiVersion: v1
    metadata:
      name: containerd-logger-config
      namespace: kube-system
    data:
      config.json: |
        {
          "inputs": [
            {
              "type": "ETW",
              "sessionNamePrefix": "containerd",
              "cleanupOldSessions": true,
              "reuseExistingSession": true,
              "providers": [
                {
                  "providerName": "Microsoft.Virtualization.RunHCS",
                  "providerGuid": "0B52781F-B24D-5685-DDF6-69830ED40EC3",
                  "level": "Verbose"
                },
                {
                  "providerName": "ContainerD",
                  "providerGuid": "2acb92c0-eb9b-571a-69cf-8f3410f383ad",
                  "level": "Verbose"
                }
              ]
            }
          ],
          "filters": [
            {
                "type": "drop",
                "include": "ProviderName == Microsoft.Virtualization.RunHCS && name == Stats && hasnoproperty error"
            },
            {
                "type": "drop",
                "include": "ProviderName == Microsoft.Virtualization.RunHCS && name == hcsshim::LayerID && hasnoproperty error"
            },
            {
                "type": "drop",
                "include": "ProviderName == Microsoft.Virtualization.RunHCS && name == hcsshim::NameToGuid && hasnoproperty error"
            },
            {
                "type": "drop",
                "include": "ProviderName == Microsoft.Virtualization.RunHCS && name == containerd.task.v2.Task.Stats && hasnoproperty error"
            },
            {
                "type": "drop",
                "include": "ProviderName == Microsoft.Virtualization.RunHCS && name == containerd.task.v2.Task.State && hasnoproperty error"
            },
            {
                "type": "drop",
                "include": "ProviderName == Microsoft.Virtualization.RunHCS && name == HcsGetProcessProperties && hasnoproperty error"
            },
            {
                "type": "drop",
                "include": "ProviderName == Microsoft.Virtualization.RunHCS && name == HcsGetComputeSystemProperties && hasnoproperty error"
            }
          ],
          "outputs": [
            {
              "type": "StdOutput"
            }
          ],
          "schemaVersion": "2016-08-11"
        }
kind: ConfigMap
metadata:
  annotations:
    note: generated
  labels:
    type: generated
  name: containerd-logger-${CLUSTER_NAME}
  namespace: default
---
apiVersion: addons.cluster.x-k8s.io/v1beta1
kind: ClusterResourceSet
metadata:
  name: metrics-server-${CLUSTER_NAME}
  namespace: default
spec:
  clusterSelector:
    matchLabels:
      metrics-server: enabled
  resources:
  - kind: ConfigMap
    name: metrics-server-${CLUSTER_NAME}
  strategy: ApplyOnce
---
apiVersion: v1
data:
  metrics-server: |
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      labels:
        k8s-app: metrics-server
      name: metrics-server
      namespace: kube-system
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      labels:
        k8s-app: metrics-server
        rbac.authorization.k8s.io/aggregate-to-admin: "true"
        rbac.authorization.k8s.io/aggregate-to-edit: "true"
        rbac.authorization.k8s.io/aggregate-to-view: "true"
      name: system:aggregated-metrics-reader
    rules:
    - apiGroups:
      - metrics.k8s.io
      resources:
      - pods
      - nodes
      verbs:
      - get
      - list
      - watch
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      labels:
        k8s-app: metrics-server
      name: system:metrics-server
    rules:
    - apiGroups:
      - ""
      resources:
      - pods
      - nodes
      - nodes/stats
      - namespaces
      - configmaps
      verbs:
      - get
      - list
      - watch
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      labels:
        k8s-app: metrics-server
      name: metrics-server-auth-reader
      namespace: kube-system
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: extension-apiserver-authentication-reader
    subjects:
    - kind: ServiceAccount
      name: metrics-server
      namespace: kube-system
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      labels:
        k8s-app: metrics-server
      name: metrics-server:system:auth-delegator
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: system:auth-delegator
    subjects:
    - kind: ServiceAccount
      name: metrics-server
      namespace: kube-system
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      labels:
        k8s-app: metrics-server
      name: system:metrics-server
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: system:metrics-server
    subjects:
    - kind: ServiceAccount
      name: metrics-server
      namespace: kube-system
    ---
    apiVersion: v1
    kind: Service
    metadata:
      labels:
        k8s-app: metrics-server
      name: metrics-server
      namespace: kube-system
    spec:
      ports:
      - name: https
        port: 443
        protocol: TCP
        targetPort: https
      selector:
        k8s-app: metrics-server
    ---
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        k8s-app: metrics-server
      name: metrics-server
      namespace: kube-system
    spec:
      selector:
        matchLabels:
          k8s-app: metrics-server
      strategy:
        rollingUpdate:
          maxUnavailable: 0
      template:
        metadata:
          labels:
            k8s-app: metrics-server
        spec:
          containers:
          - args:
            - --cert-dir=/tmp
            - --secure-port=4443
            - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
            - --kubelet-use-node-status-port
            - --metric-resolution=15s
            - --kubelet-insecure-tls
            image: k8s.gcr.io/metrics-server/metrics-server:v0.5.2
            imagePullPolicy: IfNotPresent
            livenessProbe:
              failureThreshold: 3
              httpGet:
                path: /livez
                port: https
                scheme: HTTPS
              periodSeconds: 10
            name: metrics-server
            ports:
            - containerPort: 4443
              name: https
              protocol: TCP
            readinessProbe:
              failureThreshold: 3
              httpGet:
                path: /readyz
                port: https
                scheme: HTTPS
              initialDelaySeconds: 20
              periodSeconds: 10
            resources:
              requests:
                cpu: 100m
                memory: 200Mi
            securityContext:
              readOnlyRootFilesystem: true
              runAsNonRoot: true
              runAsUser: 1000
            volumeMounts:
            - mountPath: /tmp
              name: tmp-dir
          nodeSelector:
            kubernetes.io/os: linux
          priorityClassName: system-cluster-critical
          serviceAccountName: metrics-server
          tolerations:
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
            operator: Exists
          - effect: NoSchedule
            key: node-role.kubernetes.io/control-plane
            operator: Exists
          volumes:
          - emptyDir: {}
            name: tmp-dir
    ---
    apiVersion: apiregistration.k8s.io/v1
    kind: APIService
    metadata:
      labels:
        k8s-app: metrics-server
      name: v1beta1.metrics.k8s.io
    spec:
      group: metrics.k8s.io
      groupPriorityMinimum: 100
      insecureSkipTLSVerify: true
      service:
        name: metrics-server
        namespace: kube-system
      version: v1beta1
      versionPriority: 100
kind: ConfigMap
metadata:
  annotations:
    note: generated
  labels:
    type: generated
  name: metrics-server-${CLUSTER_NAME}
  namespace: default
